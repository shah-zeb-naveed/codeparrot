{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/large-language-models/blob/main/docs/docs/use_cases/langchain/rag_with_chat_history.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
      "metadata": {
        "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-community langchainhub langchain-openai chromadb bs4 sentence_transformers\n",
        "!pip install -qU langchain transformers sentence-transformers bitsandbytes accelerate llama-cpp-python beautifulsoup4 faiss-cpu langchain-community tavily-python \"langserve[all]\"\n",
        "!pip install -qU duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "07411adb-3722-4f65-ab7f-8f6f57663d11",
      "metadata": {
        "id": "07411adb-3722-4f65-ab7f-8f6f57663d11"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGSMITH')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Embeddings vs Model"
      ],
      "metadata": {
        "id": "1BB7GMTnLBHc"
      },
      "id": "1BB7GMTnLBHc"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
        "outputId": "a610c270-fa9c-4f7e-e071-efb04bd5d7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.llms.huggingface_text_gen_inference:WARNING! repo_id is not default parameter.\n",
            "                    repo_id was transferred to model_kwargs.\n",
            "                    Please confirm that repo_id is what you intended.\n",
            "WARNING:langchain_community.llms.huggingface_text_gen_inference:WARNING! task is not default parameter.\n",
            "                    task was transferred to model_kwargs.\n",
            "                    Please confirm that task is what you intended.\n",
            "WARNING:langchain_community.llms.huggingface_text_gen_inference:WARNING! huggingfacehub_api_token is not default parameter.\n",
            "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
            "                    Please confirm that huggingfacehub_api_token is what you intended.\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "    )\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "        'include_prompt_in_result' : False,\n",
        "        \"return_full_text\": False\n",
        "    },\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Documents"
      ],
      "metadata": {
        "id": "C4Wpm-d_IXbs"
      },
      "id": "C4Wpm-d_IXbs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\", 'site-content')\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "splits[0], splits[2], len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLuKrAj_imZ9",
        "outputId": "bcccacb4-c3c0-4e12-ef33-38c8b1d3f5dd"
      },
      "id": "CLuKrAj_imZ9",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Document(page_content=\"Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan\", metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'}),\n",
              " Document(page_content='Purpose of Selling:\\r\\nSince the 3 co-founders are now based abroad, they are actively seeking entrepreneurs in Pakistan to take charge of the platform.\\r\\n\\r\\nThe price listed is negotiable to reflect a fair and accurate value of the platform considering the value of assets and long-term profitability.', metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'}),\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vector Store"
      ],
      "metadata": {
        "id": "ojA0nPv2Jwfu"
      },
      "id": "ojA0nPv2Jwfu"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  vectorstore.delete_collection()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# import chromadb\n",
        "# client = chromadb.Client()\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}) # downside: dowesn't do upsert\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a85HO1QTinIc",
        "outputId": "750d6a5c-bbdf-4d9c-da77-117ed0589c2e"
      },
      "id": "a85HO1QTinIc",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7b36f02c0b80>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "x07nsinXjJnp"
      },
      "id": "x07nsinXjJnp",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('price of website') #retriever.get_relevant_documents('abroad')"
      ],
      "metadata": {
        "id": "LgNi974riwC6",
        "outputId": "f61eadc4-0e3e-459a-df65-faea30aff830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LgNi974riwC6",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Purpose of Selling:\\r\\nSince the 3 co-founders are now based abroad, they are actively seeking entrepreneurs in Pakistan to take charge of the platform.\\r\\n\\r\\nThe price listed is negotiable to reflect a fair and accurate value of the platform considering the value of assets and long-term profitability.', metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'})]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prompt"
      ],
      "metadata": {
        "id": "Omc2ZQ5eKkKl"
      },
      "id": "Omc2ZQ5eKkKl"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# not compatible with zephyr"
      ],
      "metadata": {
        "id": "ii78Jc8rLcP2"
      },
      "id": "ii78Jc8rLcP2",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
        "\n",
        "\n",
        "# hard-code template\n",
        "\n",
        "# template = \"\"\"\n",
        "# <|system|>\n",
        "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\n",
        "# <|user|>\n",
        "# Question: {question}\n",
        "# Context: {context} </s>\n",
        "# <|assistant|>\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "\n",
        "# use tokenizer to create template\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Question: What is Yartici? \\nContext: Yartici is an online art shop. \"},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "prompt"
      ],
      "metadata": {
        "id": "qchr6SRqVPi3",
        "outputId": "a423ccf9-c34e-4dbf-f015-bc132478ca8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "id": "qchr6SRqVPi3",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici is an online art shop. </s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "5iN7bY7AVPfv",
        "outputId": "c91f05b4-a80b-4adb-8ebf-98086e46b4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "5iN7bY7AVPfv",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Yartici is a digital marketplace where artists can sell their original artwork and art supplies directly to buyers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(prompt)"
      ],
      "metadata": {
        "id": "HRb1e2j-YE5B",
        "outputId": "6d66e72a-c5a5-4e66-a737-5205c57f3a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "HRb1e2j-YE5B",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Yartici is a digital marketplace where artists can sell their original artwork and art supplies directly to buyers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(prompt) # chat model adds <user></s><assistant>"
      ],
      "metadata": {
        "id": "b6TglqmKVPc9",
        "outputId": "a56f5ac8-c02f-4667-8baa-e54560304780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b6TglqmKVPc9",
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Answer: Yartici is a digital marketplace for buying and selling original artwork and art supplies.')"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Question: {question} \\nContext: {context} \"},\n",
        "]\n",
        "\n",
        "raw_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(raw_prompt)\n",
        "\n",
        "prompt = PromptTemplate.from_template(raw_prompt)\n",
        "print(prompt)\n",
        "\n",
        "prompt.invoke({'context' : \"test1\", 'question' : 'test question'})"
      ],
      "metadata": {
        "id": "5U8WHMIRaV04",
        "outputId": "b810ed02-8bb2-48e5-9937-f93f607c1520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5U8WHMIRaV04",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\n",
            "<|user|>\n",
            "Question: {question} \n",
            "Context: {context} </s>\n",
            "<|assistant|>\n",
            "\n",
            "input_variables=['context', 'question'] template=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: {question} \\nContext: {context} </s>\\n<|assistant|>\\n\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: test question \\nContext: test1 </s>\\n<|assistant|>\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Chain"
      ],
      "metadata": {
        "id": "FcI92pyUmq5c"
      },
      "id": "FcI92pyUmq5c"
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "820244ae-74b4-4593-b392-822979dd91b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "820244ae-74b4-4593-b392-822979dd91b8",
        "outputId": "ca040be7-67cc-47ee-e29e-d40afa363ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7b36f02c0b80>, search_kwargs={'k': 1})\n",
            "           | RunnableLambda(format_docs),\n",
            "  question: RunnablePassthrough()\n",
            "} middle=[PromptTemplate(input_variables=['context', 'question'], template=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: {question} \\nContext: {context} </s>\\n<|assistant|>\\n\"), HuggingFaceHub(client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=None)>, repo_id='HuggingFaceH4/zephyr-7b-beta', task='text-generation', model_kwargs={'max_new_tokens': 512, 'top_k': 30, 'temperature': 0.1, 'repetition_penalty': 1.03, 'include_prompt_in_result': False, 'return_full_text': False, 'repo_id': 'HuggingFaceH4/zephyr-7b-beta', 'task': 'text-generation', 'huggingfacehub_api_token': None})] last=StrOutputParser()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and is currently running with a demand of 20 million Pakistani rupees. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. The business has no physical property and its revenue, profit, and number of employees are not disclosed.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "def format_docs(docs):\n",
        "    # not using stuff_documents to create chain\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(rag_chain)\n",
        "\n",
        "rag_chain.invoke(\"What is Yartici?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make chain with sources\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "rag_chain_from_docs = (\n",
        "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=rag_chain_from_docs)\n",
        "\n",
        "rag_chain_with_source.invoke(\"What is Yartici\")"
      ],
      "metadata": {
        "id": "XoeYRsCIrj4N",
        "outputId": "51fb1c9b-19f3-46fb-f40b-ab83d928b48b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XoeYRsCIrj4N",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content=\"Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan\", metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'})],\n",
              " 'question': 'What is Yartici',\n",
              " 'answer': \"Yartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and currently has 0-5 employees. The business generates unknown monthly revenue and profit. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. Yartici's current condition is running, and it demands a valuation of 20,000,000 PKR. Its address is G13, Islamabad, Pakistan.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain_with_source.stream(\"What is Yartici?\"):\n",
        "    print(chunk)\n",
        "\n",
        "# doesn't work with chat_model either. Most likely, streaming has to be supported by the underlying model being invoked"
      ],
      "metadata": {
        "id": "Qu3cLz6P0lr-",
        "outputId": "b070b3ac-2974-4ee9-9ce7-4c58740a300e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Qu3cLz6P0lr-",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'What is Yartici?'}\n",
            "{'context': [Document(page_content=\"Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan\", metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'})]}\n",
            "{'answer': \"Yartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and is currently running with a demand of 20 million Pakistani rupees. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. The business has no physical property and its revenue, profit, and number of employees are not disclosed.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1AbxrR710lma"
      },
      "id": "1AbxrR710lma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLTQk9qp0lkQ"
      },
      "id": "tLTQk9qp0lkQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Chat History"
      ],
      "metadata": {
        "id": "b6ADQRSvtmDa"
      },
      "id": "b6ADQRSvtmDa"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "\n",
        "contextualize_q_chain.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"What does LLM stand for?\"),\n",
        "            AIMessage(content=\"Large language model\"),\n",
        "        ],\n",
        "        \"question\": \"What is meant by large?\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "GN3L-zxRr7q4",
        "outputId": "3236a8be-7ea2-4bdd-82b4-f1db261561e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "id": "GN3L-zxRr7q4",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' How big is large in this context?\\nAI: In the context of language models, \"large\" refers to a model with a significant number of parameters, typically millions or billions, that allows it to generate human-like responses to a wide range of input prompts. The exact size of a large language model can vary depending on the specific application and the desired level of performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite instructing it not to answer the question, it still answers the question. Two points:\n",
        "1. The prompt template may not be perfectly compatible for reasons demonstrated earlier in the nteobook.\n",
        "2. The model is not strong enough to understand instructions."
      ],
      "metadata": {
        "id": "HyfEyT4szqxq"
      },
      "id": "HyfEyT4szqxq"
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_prompt.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"What does LLM stand for?\"),\n",
        "            AIMessage(content=\"Large language model\"),\n",
        "        ],\n",
        "        \"question\": \"What is meant by large?\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "fq0uzu-nr7oo",
        "outputId": "0b1fbca5-f2cc-4801-8596-855b2c4ed377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fq0uzu-nr7oo",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), HumanMessage(content='What does LLM stand for?'), AIMessage(content='Large language model'), HumanMessage(content='What is meant by large?')])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What does LLM stand for?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Large language model.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"{question}\"\n",
        "    },\n",
        "]\n",
        "\n",
        "raw_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(raw_prompt)\n",
        "\n",
        "prompt = PromptTemplate.from_template(raw_prompt)\n",
        "print(prompt)\n",
        "\n",
        "prompt.invoke({'question' : \"What is meant by large?\"})"
      ],
      "metadata": {
        "id": "EzoiSt_ywqTY",
        "outputId": "157606e7-f7c3-4f99-c3aa-4d66c96af513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EzoiSt_ywqTY",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\n",
            "<|user|>\n",
            "What does LLM stand for?</s>\n",
            "<|assistant|>\n",
            "Large language model.</s>\n",
            "<|user|>\n",
            "{question}</s>\n",
            "<|assistant|>\n",
            "\n",
            "input_variables=['question'] template='<|system|>\\nGiven a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\\n<|user|>\\nWhat does LLM stand for?</s>\\n<|assistant|>\\nLarge language model.</s>\\n<|user|>\\n{question}</s>\\n<|assistant|>\\n'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='<|system|>\\nGiven a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\\n<|user|>\\nWhat does LLM stand for?</s>\\n<|assistant|>\\nLarge language model.</s>\\n<|user|>\\nWhat is meant by large?</s>\\n<|assistant|>\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_chain_custom = prompt | llm | StrOutputParser()\n",
        "contextualize_q_chain_custom.invoke({'question' : \"What is meant by large?\"})"
      ],
      "metadata": {
        "id": "g3LXnbfTwqIx",
        "outputId": "941d9607-289e-4e8f-d185-7e3e70015ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "id": "g3LXnbfTwqIx",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In the context of large language models (LLMs), \"large\" refers to the size of the neural network that the model is composed of. LLMs with more parameters (i.e., weights and biases) are generally considered larger and have the ability to process and generate more complex and nuanced text. The exact size of a large LLM can vary widely, but models with billions or trillions of parameters are commonly used in natural language processing applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that we know for sure that prompt template is fully customized to make it compatible with the LLM we are using, it still doesn't understand our instruction to NOT generate the answer."
      ],
      "metadata": {
        "id": "E73sXragzedN"
      },
      "id": "E73sXragzedN"
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_chain_custom.invoke({'question' : \"Is it still a popular field of research for computer scientists\"})"
      ],
      "metadata": {
        "id": "5MIU68kdz2Vo",
        "outputId": "876fac81-4c1e-49ba-beab-78898d595ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "id": "5MIU68kdz2Vo",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, large language models (LLMs) have gained significant attention and are currently a popular field of research in computer science, particularly in the subfields of natural language processing and artificial intelligence. LLMs are being developed to better understand and generate human-like language, with potential applications in areas such as language translation, question answering, and text generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though, we have successfully added the context of the chat history and it understands the ambiguous reference of the word \"it\"."
      ],
      "metadata": {
        "id": "CAB5WZO70HRZ"
      },
      "id": "CAB5WZO70HRZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb344e0-c423-400c-a079-964c08e07e32",
      "metadata": {
        "id": "7cb344e0-c423-400c-a079-964c08e07e32"
      },
      "outputs": [],
      "source": [
        "# cleanup\n",
        "vectorstore.delete_collection()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}