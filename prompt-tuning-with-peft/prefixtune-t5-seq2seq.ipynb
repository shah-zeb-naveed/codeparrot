{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7590407,"sourceType":"datasetVersion","datasetId":4418350},{"sourceId":7590502,"sourceType":"datasetVersion","datasetId":4418427}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prefix tuning for conditional generation","metadata":{}},{"cell_type":"markdown","source":"!pip install -q peft transformers datasetsPrefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or *prefix*. Only the prefix parameters are optimized and added to the hidden states in every layer of the model. The tokens of the input sequence can still attend to the prefix as *virtual tokens*. As a result, prefix tuning stores 1000x fewer parameters than a fully finetuned model, which means you can use one large language model for many tasks.\n\n<Tip>\n\nðŸ’¡ Read [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190) to learn more about prefix tuning. \n\n</Tip>\n\nThis guide will show you how to apply prefix tuning to train a [`t5-large`](https://huggingface.co/t5-large) model on the `sentences_allagree` subset of the [financial_phrasebank](https://huggingface.co/datasets/financial_phrasebank) dataset.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\n!pip install -q peft transformers datasets\n```","metadata":{}},{"cell_type":"code","source":"!pip install -q peft transformers datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"markdown","source":"Start by defining the model and tokenizer, text and label columns, and some hyperparameters so it'll be easier to start training faster later. Set the environment variable `TOKENIZERS_PARALLELSIM` to `false` to disable the fast Rust-based tokenizer which processes data in parallel by default so you can use multiprocessing in Python.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\nfrom peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\ndevice = \"cuda\"\nmodel_name_or_path = \"t5-large\"\ntokenizer_name_or_path = \"t5-large\"\n\nmax_length = 128\nlr = 1e-2\nnum_epochs = 5\nbatch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:37:06.136369Z","iopub.execute_input":"2024-02-08T17:37:06.137150Z","iopub.status.idle":"2024-02-08T17:37:11.716333Z","shell.execute_reply.started":"2024-02-08T17:37:06.137115Z","shell.execute_reply":"2024-02-08T17:37:11.715473Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-08 17:37:08.117046: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-08 17:37:08.117110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-08 17:37:08.118652: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"markdown","source":"For this guide, you'll train on the `sentences_allagree` subset of the [`financial_phrasebank`](https://huggingface.co/datasets/financial_phrasebank) dataset. This dataset contains financial news categorized by sentiment.\n\nUse ðŸ¤— [Datasets](https://huggingface.co/docs/datasets/index) [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) function to create a training and validation split and convert the `label` value to the more readable `text_label`. All of the changes can be applied with the [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function:","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# dataset_name = \"twitter_complaints\"\n# dataset = load_dataset(\"ought/raft\", dataset_name)\ndataset = load_dataset('/kaggle/input/loader', 'sentences_allagree')\n\n#dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\ndataset = dataset[\"train\"].train_test_split(test_size=0.1)\ndataset[\"validation\"] = dataset[\"test\"]\ndel dataset[\"test\"]\n\ntext_column = \"sentence\"\nlabel_column = \"text_label\"\nclasses = dataset[\"train\"].features[\"label\"].names","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:22:51.950933Z","iopub.execute_input":"2024-02-08T18:22:51.951301Z","iopub.status.idle":"2024-02-08T18:22:51.983774Z","shell.execute_reply.started":"2024-02-08T18:22:51.951274Z","shell.execute_reply":"2024-02-08T18:22:51.983126Z"},"trusted":true},"execution_count":108,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd7610bd4c244a08ce67baf40387fed"}},"metadata":{}},{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label'],\n        num_rows: 2264\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.map(\n    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n    batched=True,\n    num_proc=1,\n)\n\ndataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:23:43.841575Z","iopub.execute_input":"2024-02-08T18:23:43.842424Z","iopub.status.idle":"2024-02-08T18:23:43.915204Z","shell.execute_reply.started":"2024-02-08T18:23:43.842390Z","shell.execute_reply":"2024-02-08T18:23:43.914529Z"},"trusted":true},"execution_count":111,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0312e05d6c6641648d85c7ea22662a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac663dfc6db48af96b09a98342d1604"}},"metadata":{}},{"execution_count":111,"output_type":"execute_result","data":{"text/plain":"{'sentence': 'The remainder of its revenues will come from technology agreements with other firms , InterDigital said .',\n 'label': 1,\n 'text_label': 'neutral'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocess dataset","metadata":{}},{"cell_type":"markdown","source":"Initialize a tokenizer, and create a function to pad and truncate the `model_inputs` and `labels`:","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:24:07.745294Z","iopub.execute_input":"2024-02-08T18:24:07.746047Z","iopub.status.idle":"2024-02-08T18:24:08.360761Z","shell.execute_reply.started":"2024-02-08T18:24:07.746011Z","shell.execute_reply":"2024-02-08T18:24:08.359949Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Use the [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function to apply the `preprocess_function` to the dataset. You can remove the unprocessed columns since the model doesn't need them anymore:","metadata":{}},{"cell_type":"code","source":"import numpy as np \n\ndef preprocess_function(examples):\n    inputs = examples[text_column]\n    targets = examples[label_column]\n    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True) # , return_tensors=\"pt\"\n    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True) # , return_tensors=\"pt\"\n    labels = np.array(labels[\"input_ids\"])\n    labels[labels == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels \n    return model_inputs\n\nprocessed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n\nprint(processed_datasets['train'][0])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-08T18:24:54.867066Z","iopub.execute_input":"2024-02-08T18:24:54.867731Z","iopub.status.idle":"2024-02-08T18:24:55.354163Z","shell.execute_reply.started":"2024-02-08T18:24:54.867694Z","shell.execute_reply":"2024-02-08T18:24:55.353507Z"},"trusted":true},"execution_count":115,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056c474c9228473b996df71e8f9eaf2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dfc64864abf4b4ebc76f1a7c0a4f823"}},"metadata":{}}]},{"cell_type":"markdown","source":"Create a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) from the `train` and `eval` datasets. Set `pin_memory=True` to speed up the data transfer to the GPU during training if the samples in your dataset are on a CPU.","metadata":{}},{"cell_type":"code","source":"train_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"validation\"]\n\ntrain_dataloader = DataLoader(\n    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:25:03.675949Z","iopub.execute_input":"2024-02-08T18:25:03.676858Z","iopub.status.idle":"2024-02-08T18:25:03.681339Z","shell.execute_reply.started":"2024-02-08T18:25:03.676821Z","shell.execute_reply":"2024-02-08T18:25:03.680718Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"markdown","source":"Now you can setup your model and make sure it is ready for training. Specify the task in [PrefixTuningConfig](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PrefixTuningConfig), create the base `t5-large` model from [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM), and then wrap the model and configuration in a [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel). Feel free to print the [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel)'s parameters and compare it to fully training all the model parameters to see how much more efficient it is!","metadata":{}},{"cell_type":"code","source":"peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n#\"trainable params: 983040 || all params: 738651136 || trainable%: 0.13308583065659835\"","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:25:11.069301Z","iopub.execute_input":"2024-02-08T18:25:11.070096Z","iopub.status.idle":"2024-02-08T18:25:12.981676Z","shell.execute_reply.started":"2024-02-08T18:25:11.070060Z","shell.execute_reply":"2024-02-08T18:25:12.981014Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:38:08.491804Z","iopub.execute_input":"2024-02-08T18:38:08.492914Z","iopub.status.idle":"2024-02-08T18:38:08.506608Z","shell.execute_reply.started":"2024-02-08T18:38:08.492873Z","shell.execute_reply":"2024-02-08T18:38:08.505903Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 1024)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 1024)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 1024)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n                (relative_attention_bias): Embedding(32, 16)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-23): 23 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=1024, out_features=1024, bias=False)\n                (k): Linear(in_features=1024, out_features=1024, bias=False)\n                (v): Linear(in_features=1024, out_features=1024, bias=False)\n                (o): Linear(in_features=1024, out_features=1024, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseActDense(\n                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): ReLU()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n  )\n  (prompt_encoder): ModuleDict(\n    (default): PrefixEncoder(\n      (embedding): Embedding(20, 49152)\n    )\n  )\n  (word_embeddings): Embedding(32128, 1024)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Setup the optimizer and learning rate scheduler:","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:25:17.506780Z","iopub.execute_input":"2024-02-08T18:25:17.507171Z","iopub.status.idle":"2024-02-08T18:25:17.517859Z","shell.execute_reply.started":"2024-02-08T18:25:17.507139Z","shell.execute_reply":"2024-02-08T18:25:17.517109Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"Move the model to the GPU, and then write a training loop to begin!","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:37:30.620283Z","iopub.execute_input":"2024-02-08T17:37:30.621041Z","iopub.status.idle":"2024-02-08T17:37:30.641899Z","shell.execute_reply.started":"2024-02-08T17:37:30.621008Z","shell.execute_reply":"2024-02-08T17:37:30.641182Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7af22e2184d41eea8c2523ab93967a4"}},"metadata":{}}]},{"cell_type":"code","source":"peft_model_id = \"shahzebnaveed/t5-large_PREFIX_TUNING_SEQ2SEQ\"","metadata":{"execution":{"iopub.status.busy":"2024-02-08T17:44:16.488380Z","iopub.execute_input":"2024-02-08T17:44:16.489306Z","iopub.status.idle":"2024-02-08T17:44:20.155847Z","shell.execute_reply.started":"2024-02-08T17:44:16.489271Z","shell.execute_reply":"2024-02-08T17:44:20.155192Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/3.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38819bcd4c21487fb829e722dd268908"}},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shahzebnaveed/t5-large_PREFIX_TUNING_SEQ2SEQ/commit/b228bac19e3ebd7a7a650ed1c8af2f9fd0b38e93', commit_message='Upload model', commit_description='', oid='b228bac19e3ebd7a7a650ed1c8af2f9fd0b38e93', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"model = model.to(device)\n\nfor epoch in range(10):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n    \n    if epoch in [3, 4, 10]:\n        model.push_to_hub(peft_model_id, use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:27:15.251358Z","iopub.execute_input":"2024-02-08T18:27:15.252176Z","iopub.status.idle":"2024-02-08T18:35:05.236238Z","shell.execute_reply.started":"2024-02-08T18:27:15.252140Z","shell.execute_reply":"2024-02-08T18:35:05.235522Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  5.99it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.15it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  6.00it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.18it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  6.01it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.17it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  6.00it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.18it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/3.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9251555ff53242a69788ccf01eaaaf8f"}},"metadata":{}},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  6.02it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.14it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  5.99it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.19it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  5.94it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.17it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  5.98it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.13it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  6.01it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.17it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:42<00:00,  6.01it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:04<00:00,  7.17it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's see how well the model performs on the validation set:","metadata":{}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nfor pred, true in zip(eval_preds, dataset[\"validation\"][\"text_label\"]):\n    if pred.strip() == true.strip():\n        correct += 1\n    total += 1\naccuracy = correct / total * 100\naccuracy","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:35:14.155018Z","iopub.execute_input":"2024-02-08T18:35:14.155408Z","iopub.status.idle":"2024-02-08T18:35:14.162930Z","shell.execute_reply.started":"2024-02-08T18:35:14.155374Z","shell.execute_reply":"2024-02-08T18:35:14.162201Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"96.91629955947137"},"metadata":{}}]},{"cell_type":"code","source":"# import pandas as pd\n# pd.DataFrame(zip(dataset['train']['text_label'], dataset['train']['Tweet text'])).head()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:35:20.795187Z","iopub.execute_input":"2024-02-08T18:35:20.795892Z","iopub.status.idle":"2024-02-08T18:35:20.799131Z","shell.execute_reply.started":"2024-02-08T18:35:20.795855Z","shell.execute_reply":"2024-02-08T18:35:20.798456Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"97% accuracy in just a few minutes; pretty good!","metadata":{}},{"cell_type":"markdown","source":"Upload the model to a specifc model repository on the Hub with the [push_to_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) function:","metadata":{}},{"cell_type":"code","source":"model.push_to_hub(peft_model_id, use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:35:27.493556Z","iopub.execute_input":"2024-02-08T18:35:27.494364Z","iopub.status.idle":"2024-02-08T18:35:30.219101Z","shell.execute_reply.started":"2024-02-08T18:35:27.494328Z","shell.execute_reply":"2024-02-08T18:35:30.218334Z"},"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shahzebnaveed/t5-large_PREFIX_TUNING_SEQ2SEQ/commit/3ea257e53ecac100b48562e9ca80c8d9f9bbe66c', commit_message='Upload model', commit_description='', oid='3ea257e53ecac100b48562e9ca80c8d9f9bbe66c', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"markdown","source":"Once the model has been uploaded to the Hub, anyone can easily use it for inference. Load the configuration and model:","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_id = \"shahzebnaveed/t5-large_PREFIX_TUNING_SEQ2SEQ\"\n\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:35:35.477369Z","iopub.execute_input":"2024-02-08T18:35:35.477801Z","iopub.status.idle":"2024-02-08T18:35:38.036138Z","shell.execute_reply.started":"2024-02-08T18:35:35.477761Z","shell.execute_reply":"2024-02-08T18:35:38.035476Z"},"trusted":true},"execution_count":125,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/3.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecde88fe8d7e442d8e7a97c2bfd8e766"}},"metadata":{}}]},{"cell_type":"markdown","source":"Get and tokenize some text about financial news:","metadata":{}},{"cell_type":"code","source":"dataset['validation'][4]['sentence']","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:36:55.575492Z","iopub.execute_input":"2024-02-08T18:36:55.575954Z","iopub.status.idle":"2024-02-08T18:36:55.581497Z","shell.execute_reply.started":"2024-02-08T18:36:55.575915Z","shell.execute_reply":"2024-02-08T18:36:55.580811Z"},"trusted":true},"execution_count":133,"outputs":[{"execution_count":133,"output_type":"execute_result","data":{"text/plain":"'Operating loss was EUR 179mn , compared to a loss of EUR 188mn in the second quarter of 2009 .'"},"metadata":{}}]},{"cell_type":"code","source":"inputs = tokenizer(\n    dataset['validation'][4]['sentence'],\n    return_tensors=\"pt\",\n)\n\nmodel.to(device)\n\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=2)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n    tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\ntokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-08T18:37:03.737950Z","iopub.execute_input":"2024-02-08T18:37:03.738341Z","iopub.status.idle":"2024-02-08T18:37:03.845105Z","shell.execute_reply.started":"2024-02-08T18:37:03.738298Z","shell.execute_reply":"2024-02-08T18:37:03.844428Z"},"trusted":true},"execution_count":135,"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":"['negative']"},"metadata":{}}]}]}