{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
      "metadata": {
        "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-community langchainhub langchain-openai chromadb bs4 sentence_transformers transformers\n",
        "!pip install -qU bitsandbytes accelerate llama-cpp-python beautifulsoup4 faiss-cpu tavily-python \"langserve[all]\"\n",
        "!pip install -qU duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "07411adb-3722-4f65-ab7f-8f6f57663d11",
      "metadata": {
        "id": "07411adb-3722-4f65-ab7f-8f6f57663d11"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    AIMessage\n",
        ")\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGSMITH')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Embeddings vs Model"
      ],
      "metadata": {
        "id": "1BB7GMTnLBHc"
      },
      "id": "1BB7GMTnLBHc"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
        "outputId": "e7344cb9-ba0c-40e0-98c0-8cbee5789229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.llms.huggingface_text_gen_inference:WARNING! repo_id is not default parameter.\n",
            "                    repo_id was transferred to model_kwargs.\n",
            "                    Please confirm that repo_id is what you intended.\n",
            "WARNING:langchain_community.llms.huggingface_text_gen_inference:WARNING! task is not default parameter.\n",
            "                    task was transferred to model_kwargs.\n",
            "                    Please confirm that task is what you intended.\n",
            "WARNING:langchain_community.llms.huggingface_text_gen_inference:WARNING! huggingfacehub_api_token is not default parameter.\n",
            "                    huggingfacehub_api_token was transferred to model_kwargs.\n",
            "                    Please confirm that huggingfacehub_api_token is what you intended.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
        "\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "    )\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "        'include_prompt_in_result' : True,\n",
        "        \"return_full_text\": True\n",
        "    },\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Documents"
      ],
      "metadata": {
        "id": "C4Wpm-d_IXbs"
      },
      "id": "C4Wpm-d_IXbs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\", 'site-content')\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "splits[0], splits[2], len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLuKrAj_imZ9",
        "outputId": "0ab57767-a8be-424e-d19c-d2a6dd25271b"
      },
      "id": "CLuKrAj_imZ9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Document(page_content=\"Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan\", metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'}),\n",
              " Document(page_content='Purpose of Selling:\\r\\nSince the 3 co-founders are now based abroad, they are actively seeking entrepreneurs in Pakistan to take charge of the platform.\\r\\n\\r\\nThe price listed is negotiable to reflect a fair and accurate value of the platform considering the value of assets and long-term profitability.', metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'}),\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vector Store"
      ],
      "metadata": {
        "id": "ojA0nPv2Jwfu"
      },
      "id": "ojA0nPv2Jwfu"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  vectorstore.delete_collection()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# import chromadb\n",
        "# client = chromadb.Client()\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}) # downside: dowesn't do upsert\n",
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a85HO1QTinIc",
        "outputId": "b1c4828e-d0ca-4012-9ee7-e7ff211b869c"
      },
      "id": "a85HO1QTinIc",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7ee06cdbd270>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "x07nsinXjJnp"
      },
      "id": "x07nsinXjJnp",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('price of website') #retriever.get_relevant_documents('abroad')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgNi974riwC6",
        "outputId": "b27c06ec-b242-44aa-91a4-ac051748c779"
      },
      "id": "LgNi974riwC6",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Purpose of Selling:\\r\\nSince the 3 co-founders are now based abroad, they are actively seeking entrepreneurs in Pakistan to take charge of the platform.\\r\\n\\r\\nThe price listed is negotiable to reflect a fair and accurate value of the platform considering the value of assets and long-term profitability.', metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'})]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prompt"
      ],
      "metadata": {
        "id": "Omc2ZQ5eKkKl"
      },
      "id": "Omc2ZQ5eKkKl"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ii78Jc8rLcP2"
      },
      "id": "ii78Jc8rLcP2",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not compatible with zephyr\n",
        "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# hard-code template\n",
        "# template = \"\"\"\n",
        "# <|system|>\n",
        "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\n",
        "# <|user|>\n",
        "# Question: {question}\n",
        "# Context: {context} </s>\n",
        "# <|assistant|>\n",
        "# Answer:\n",
        "# \"\"\"\n",
        "\n",
        "SYSTEM_MESSAGE = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\n",
        "USER_MESSAGE = \"Question: What is Yartici? \\nContext: Yartici is an online art shop. \"\n",
        "\n",
        "# use tokenizer to create template\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_MESSAGE,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": USER_MESSAGE},\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "qchr6SRqVPi3",
        "outputId": "974710a5-edd8-4671-98e8-80d5d46f67a9"
      },
      "id": "qchr6SRqVPi3",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici is an online art shop. </s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can also use langchain modules\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=SYSTEM_MESSAGE),\n",
        "    HumanMessage(content=USER_MESSAGE),\n",
        "]\n",
        "\n",
        "# chat model understands the expected prompt template. Interesting because I previously thought it's dumb.\n",
        "chat_model._to_chat_prompt(messages)"
      ],
      "metadata": {
        "id": "gGOs4oixfpAm",
        "outputId": "582d60d8-22f4-49c6-fdea-b840e10f8be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "id": "gGOs4oixfpAm",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici is an online art shop. </s>\\n<|assistant|>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(prompt)\n",
        "# llm.predict(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "5iN7bY7AVPfv",
        "outputId": "8517509b-dcfb-43dc-b098-22da0e92476d"
      },
      "id": "5iN7bY7AVPfv",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici is an online art shop. </s>\\n<|assistant|>\\nAnswer: Yartici is a digital marketplace for purchasing original artwork and art supplies directly from artists.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(prompt) # chat_model.invoke() adds <user></s><assistant>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6TglqmKVPc9",
        "outputId": "e1e6173c-fd17-4bb6-81c8-fe72e14b9d8c"
      },
      "id": "b6TglqmKVPc9",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"<|user|>\\n<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici is an online art shop. </s>\\n<|assistant|>\\n</s>\\n<|assistant|>\\nAnswer: Yartici is a digital marketplace where artists can sell their original artwork and art supplies directly to buyers.\")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(messages) # this method works well for chat_models"
      ],
      "metadata": {
        "id": "zXKeEAEfvr1g",
        "outputId": "20278102-0493-4034-f9d1-6936a95c3157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zXKeEAEfvr1g",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici is an online art shop. </s>\\n<|assistant|>\\nAnswer: Yartici is a digital marketplace for purchasing original artwork and art supplies directly from artists.\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_MESSAGE,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Question: {question} \\nContext: {context} \"},\n",
        "]\n",
        "\n",
        "raw_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(raw_prompt)\n",
        "\n",
        "prompt = PromptTemplate.from_template(raw_prompt)\n",
        "print(prompt)\n",
        "\n",
        "prompt.invoke({'context' : \"test1\", 'question' : 'test question'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U8WHMIRaV04",
        "outputId": "2be8b56c-3214-4618-c11a-52d20413e7bc"
      },
      "id": "5U8WHMIRaV04",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\n",
            "<|user|>\n",
            "Question: {question} \n",
            "Context: {context} </s>\n",
            "<|assistant|>\n",
            "\n",
            "input_variables=['context', 'question'] template=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: {question} \\nContext: {context} </s>\\n<|assistant|>\\n\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: test question \\nContext: test1 </s>\\n<|assistant|>\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuple_messages = [\n",
        "    (\"system\", SYSTEM_MESSAGE),\n",
        "    (\"human\", \"Question: {question} \\nContext: {context} \")\n",
        "]\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(tuple_messages)\n",
        "chat_prompt"
      ],
      "metadata": {
        "id": "gzWiYBkHwrnN",
        "outputId": "acbc1f80-f2f8-4106-ebe5-e0975bf5d105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "gzWiYBkHwrnN",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Question: {question} \\nContext: {context} '))])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt.invoke({'context' : \"test1\", 'question' : 'test question'}).to_string()\n",
        "# this shows that chattemplate uses \"System\" instead of <system>\n",
        "\n",
        "# because we have only rendered the template without giving it any informationr egarding the model"
      ],
      "metadata": {
        "id": "N_uPYf1Hwrjj",
        "outputId": "e75fbc70-41b7-495e-bd3d-d5c9701281f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "id": "N_uPYf1Hwrjj",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nHuman: Question: test question \\nContext: test1 \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Chain"
      ],
      "metadata": {
        "id": "FcI92pyUmq5c"
      },
      "id": "FcI92pyUmq5c"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "820244ae-74b4-4593-b392-822979dd91b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "820244ae-74b4-4593-b392-822979dd91b8",
        "outputId": "427751f7-262e-47f6-f7f9-a203b738a0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7ee06cdbd270>, search_kwargs={'k': 1})\n",
            "           | RunnableLambda(format_docs),\n",
            "  question: RunnablePassthrough()\n",
            "} middle=[PromptTemplate(input_variables=['context', 'question'], template=\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: {question} \\nContext: {context} </s>\\n<|assistant|>\\n\"), HuggingFaceHub(client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=None)>, repo_id='HuggingFaceH4/zephyr-7b-beta', task='text-generation', model_kwargs={'max_new_tokens': 512, 'top_k': 30, 'temperature': 0.1, 'repetition_penalty': 1.03, 'include_prompt_in_result': True, 'return_full_text': True, 'repo_id': 'HuggingFaceH4/zephyr-7b-beta', 'task': 'text-generation', 'huggingfacehub_api_token': None})] last=StrOutputParser()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan </s>\\n<|assistant|>\\nYartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and currently has 0-5 employees. The business generates unknown monthly revenue and profit. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. Yartici's current condition is running, and it demands a valuation of 20 million Pakistani rupees. Its address is G13, Islamabad, Pakistan.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "def format_docs(docs):\n",
        "    # not using stuff_documents to create chain\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(rag_chain)\n",
        "\n",
        "rag_chain.invoke(\"What is Yartici?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make chain with chat\n",
        "\n",
        "def format_docs(docs):\n",
        "    # not using stuff_documents to create chain\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chat_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | chat_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(rag_chat_chain)\n",
        "\n",
        "rag_chat_chain.invoke(\"What is Yartici?\")"
      ],
      "metadata": {
        "id": "5k857IbYzE66",
        "outputId": "9d80e4d5-a33d-4336-a31e-cfedd901677a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "id": "5k857IbYzE66",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7ee06cdbd270>, search_kwargs={'k': 1})\n",
            "           | RunnableLambda(format_docs),\n",
            "  question: RunnablePassthrough()\n",
            "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Question: {question} \\nContext: {context} '))]), ChatHuggingFace(llm=HuggingFaceHub(client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=None)>, repo_id='HuggingFaceH4/zephyr-7b-beta', task='text-generation', model_kwargs={'max_new_tokens': 512, 'top_k': 30, 'temperature': 0.1, 'repetition_penalty': 1.03, 'include_prompt_in_result': True, 'return_full_text': True, 'repo_id': 'HuggingFaceH4/zephyr-7b-beta', 'task': 'text-generation', 'huggingfacehub_api_token': None}), tokenizer=LlamaTokenizerFast(name_or_path='HuggingFaceH4/zephyr-7b-beta', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}, model_id='HuggingFaceH4/zephyr-7b-beta')] last=StrOutputParser()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan </s>\\n<|assistant|>\\nYartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and currently has 0-5 employees. The business generates unknown monthly revenue and profit. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. Yartici's current condition is running, and it demands a valuation of 20 million Pakistani rupees. Its address is G13, Islamabad, Pakistan.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make chain with sources\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "doc_chain = (\n",
        "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "    | chat_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain_chat_from_docs = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ").assign(answer=doc_chain)\n",
        "\n",
        "rag_chain_chat_from_docs.invoke(\"What is Yartici\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoeYRsCIrj4N",
        "outputId": "d4292540-d342-44fb-ad2e-c1da43c75f9d"
      },
      "id": "XoeYRsCIrj4N",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content=\"Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan\", metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'})],\n",
              " 'question': 'What is Yartici',\n",
              " 'answer': \"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici \\nContext: Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan </s>\\n<|assistant|>\\nYartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and currently has 0-5 employees. The business generates unknown monthly revenue and profit. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. Yartici's current condition is running, and it demands a valuation of 20,000,000 PKR. Its address is G13, Islamabad, Pakistan.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain_chat_from_docs.stream(\"What is Yartici?\"):\n",
        "    print(chunk)\n",
        "\n",
        "# doesn't work with chat_model either. Most likely, streaming has to be supported by the underlying model being invoked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu3cLz6P0lr-",
        "outputId": "ec059014-532e-48b5-c753-2f5360793eef"
      },
      "id": "Qu3cLz6P0lr-",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'What is Yartici?'}\n",
            "{'context': [Document(page_content=\"Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan\", metadata={'source': 'https://flippers.pk/ads/yartici-com-pakistans-leading-online-art-marketplace/'})]}\n",
            "{'answer': \"<|system|>\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.</s>\\n<|user|>\\nQuestion: What is Yartici? \\nContext: Yartici.com - Pakistan's Leading Online Art Marketplace   | E-Commerce Platforms\\n\\n \\n  G13, Islamabad, Pakistan  -\\n  Islamabad\\n\\n\\n\\n\\n\\n\\n\\n\\nBusiness CategoryE-Commerce Platforms\\nBusiness Established Year2020\\nRevenue Per MonthN/A\\nMonthly ProfitN/A\\nNumber of employees0 to 5\\nProperty valueN/A\\nProperty RentN/A\\nAsset include:Website, Domain, Email, AWS Hosting, Social Media Accounts \\nAsset Value:N/A\\nCurrent ConditionRunning\\nDemand20,000,000\\nAddressG13, Islamabad, Pakistan </s>\\n<|assistant|>\\nYartici is Pakistan's leading online art marketplace, established in 2020. It operates as an e-commerce platform and currently has 0-5 employees. The business generates unknown monthly revenue and profit. Its assets include a website, domain, email, AWS hosting, and social media accounts, with an asset value of N/A. Yartici's current condition is running, and it demands a valuation of 20 million Pakistani rupees. Its address is G13, Islamabad, Pakistan.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1AbxrR710lma"
      },
      "id": "1AbxrR710lma",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tLTQk9qp0lkQ"
      },
      "id": "tLTQk9qp0lkQ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Chat History"
      ],
      "metadata": {
        "id": "b6ADQRSvtmDa"
      },
      "id": "b6ADQRSvtmDa"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"), # placeholder because .chat_history variable will comprise of a series system/human interactions.\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_q_chain = contextualize_q_prompt | chat_model | StrOutputParser()\n",
        "\n",
        "contextualize_q_chain.invoke(\n",
        "    {\n",
        "        \"chat_history\": [\n",
        "            HumanMessage(content=\"What does LLM stand for?\"),\n",
        "            AIMessage(content=\"Large language model\"),\n",
        "        ],\n",
        "        \"question\": \"What is meant by large?\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "GN3L-zxRr7q4",
        "outputId": "b514794d-576a-4a0d-d145-7acb239ea5ab"
      },
      "id": "GN3L-zxRr7q4",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nGiven a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\\n<|user|>\\nWhat does LLM stand for?</s>\\n<|assistant|>\\nLarge language model</s>\\n<|user|>\\nWhat is meant by large?</s>\\n<|assistant|>\\nIn the context of large language models (LLMs), \"large\" refers to the size of the neural network that the model is composed of. LLMs with more parameters (i.e., weights and biases) are generally considered larger and have the ability to process and generate more complex and nuanced text. The exact size of a large LLM can vary widely, but models with billions or trillions of parameters are common in the field of natural language processing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite instructing it not to answer the question, it still answers the question. Two points:\n",
        "1. The prompt template may not be perfectly compatible for reasons demonstrated earlier in the nteobook. (SOLVED: After using approporiate templating method, it is now compatible. Although this was not the issue as demonstrated below.)\n",
        "2. The model is not strong enough to understand instructions."
      ],
      "metadata": {
        "id": "HyfEyT4szqxq"
      },
      "id": "HyfEyT4szqxq"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What does LLM stand for?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Large language model.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"{question}\"\n",
        "    },\n",
        "]\n",
        "\n",
        "raw_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(raw_prompt)\n",
        "\n",
        "prompt = PromptTemplate.from_template(raw_prompt)\n",
        "print(prompt)\n",
        "\n",
        "prompt.invoke({'question' : \"What is meant by large?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzoiSt_ywqTY",
        "outputId": "dab0b775-abf8-415e-ea3e-4bf73c899e71"
      },
      "id": "EzoiSt_ywqTY",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\n",
            "<|user|>\n",
            "What does LLM stand for?</s>\n",
            "<|assistant|>\n",
            "Large language model.</s>\n",
            "<|user|>\n",
            "{question}</s>\n",
            "<|assistant|>\n",
            "\n",
            "input_variables=['question'] template='<|system|>\\nGiven a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\\n<|user|>\\nWhat does LLM stand for?</s>\\n<|assistant|>\\nLarge language model.</s>\\n<|user|>\\n{question}</s>\\n<|assistant|>\\n'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='<|system|>\\nGiven a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.</s>\\n<|user|>\\nWhat does LLM stand for?</s>\\n<|assistant|>\\nLarge language model.</s>\\n<|user|>\\nWhat is meant by large?</s>\\n<|assistant|>\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_chain_custom = prompt | llm | StrOutputParser()\n",
        "contextualize_q_chain_custom.invoke({'question' : \"What is meant by large?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "g3LXnbfTwqIx",
        "outputId": "02605153-6701-4d09-ca63-d50c7797c571"
      },
      "id": "g3LXnbfTwqIx",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In the context of large language models (LLMs), \"large\" refers to the size of the neural network that the model is composed of. LLMs with more parameters (i.e., weights and biases) are generally considered larger and have the ability to process and generate more complex and nuanced text. The exact definition of what constitutes a \"large\" LLM can vary depending on the specific application and task being performed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that we know for sure that prompt template is fully customized to make it compatible with the LLM we are using, it still doesn't understand our instruction to NOT generate the answer."
      ],
      "metadata": {
        "id": "E73sXragzedN"
      },
      "id": "E73sXragzedN"
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_chain_custom.invoke({'question' : \"Is it still a popular field of research for computer scientists\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "5MIU68kdz2Vo",
        "outputId": "dee20171-38c5-49cc-abe1-6f87627ef3b0"
      },
      "id": "5MIU68kdz2Vo",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, large language models (LLMs) have gained significant attention and are currently a popular field of research in computer science, particularly in the areas of natural language processing and artificial intelligence. LLMs are being developed to better understand and generate human-like language, with potential applications in various industries such as healthcare, finance, and education.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though, we have successfully added the context of the chat history and it understands the ambiguous reference of the word \"it\"."
      ],
      "metadata": {
        "id": "CAB5WZO70HRZ"
      },
      "id": "CAB5WZO70HRZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "Huggingface messages template is a list of dict of role/content (system, user, assistant) while the LangChain is a list SystemMessage/HumanMessages or a list of tuples (role, content) e.g. system, human, ai."
      ],
      "metadata": {
        "id": "ZWqkI-FM6Bct"
      },
      "id": "ZWqkI-FM6Bct"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7cb344e0-c423-400c-a079-964c08e07e32",
      "metadata": {
        "id": "7cb344e0-c423-400c-a079-964c08e07e32"
      },
      "outputs": [],
      "source": [
        "# cleanup\n",
        "vectorstore.delete_collection()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}