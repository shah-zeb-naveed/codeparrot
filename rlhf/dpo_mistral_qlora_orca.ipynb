{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"22773c721a7c4221a9c14cd388461d4c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b54841f5de1482694c360095dae3039","IPY_MODEL_448ccbc85e624ec3b3e71931a7ee4ff6","IPY_MODEL_173769f6f465485f8848a11bf269850b"],"layout":"IPY_MODEL_60978b9b4e8348f0a71ce3e35c73bcff"}},"6b54841f5de1482694c360095dae3039":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a38dcbaf4674b448329ac0a16587d2a","placeholder":"​","style":"IPY_MODEL_7eaeada2158e493189449af91f643553","value":"Loading checkpoint shards: 100%"}},"448ccbc85e624ec3b3e71931a7ee4ff6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e32854952b340008edca0139d3471d6","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db6d7cfcdade4b4baa213a5d0abc07d7","value":3}},"173769f6f465485f8848a11bf269850b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9083029642744c43b7705532cbe0cf79","placeholder":"​","style":"IPY_MODEL_d028a98caa13425b907ceb513119006e","value":" 3/3 [00:11&lt;00:00,  2.89s/it]"}},"60978b9b4e8348f0a71ce3e35c73bcff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a38dcbaf4674b448329ac0a16587d2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eaeada2158e493189449af91f643553":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e32854952b340008edca0139d3471d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db6d7cfcdade4b4baa213a5d0abc07d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9083029642744c43b7705532cbe0cf79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d028a98caa13425b907ceb513119006e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune a OpenHermes2.5Mistral7b model with DPO using QLORA - An Alternative Approach for RLHF\n\nThis bypasses traditional reward modelling/reinforcement learning loop in PPO and directly optimizes the policy network using a reparameterized objective function resulting in a simpler cross entropy loss.\n\n❤️ Originally created by @maximelabonne. I've tried to reduce the parameters to make it run over a smaller GPU but seems like it doesn't work.","metadata":{"id":"Pa8905-YsHAn"}},{"cell_type":"code","source":"!pip install -q -U datasets trl peft bitsandbytes sentencepiece wandb","metadata":{"id":"_zIBL8IssExG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport torch\n\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom trl import DPOTrainer\nimport bitsandbytes as bnb\n#from google.colab import userdata\nimport wandb\n\n# Defined in the secrets tab in Google Colab\n#hf_token = userdata.get('huggingface')\n#wb_token = userdata.get('wandb')\nwandb.login()#key=wb_token\n\nmodel_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"NeuralHermes-2.5-Mistral-7B\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpdkZsMNylvp","outputId":"6c2df234-1ce7-4cd2-a7e3-567e7536319f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Format dataset","metadata":{"id":"d8CvUgROUDw-"}},{"cell_type":"code","source":"def chatml_format(example):\n    # Format system\n    if len(example['system']) > 0:\n        message = {\"role\": \"system\", \"content\": example['system']}\n        system = tokenizer.apply_chat_template([message], tokenize=False)\n    else:\n        system = \"\"\n\n    # Format instruction\n    message = {\"role\": \"user\", \"content\": example['question']}\n    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n\n    # Format chosen answer\n    chosen = example['chosen'] + \"<|im_end|>\\n\"\n\n    # Format rejected answer\n    rejected = example['rejected'] + \"<|im_end|>\\n\"\n\n    return {\n        \"prompt\": system + prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected,\n    }\n\n# Load dataset\ndataset = load_dataset(\"Intel/orca_dpo_pairs\")['train']\n\n# Save columns\noriginal_columns = dataset.column_names\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# Format dataset\ndataset = dataset.map(\n    chatml_format,\n    remove_columns=original_columns\n)\n\n# Print sample\ndataset[1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCD77GZ60DOT","outputId":"c7c6773c-5545-4fee-bfa3-6fa6d69c0f3f","execution":{"iopub.status.busy":"2024-02-15T14:38:39.057021Z","iopub.execute_input":"2024-02-15T14:38:39.057810Z","iopub.status.idle":"2024-02-15T14:38:47.690558Z","shell.execute_reply.started":"2024-02-15T14:38:39.057771Z","shell.execute_reply":"2024-02-15T14:38:47.689641Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/196 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fac7a1ec7b4bc1a808d3924712cfa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/36.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a39e6180ff624bab9033094c546b7ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5ec02f5d4b4f2f80b8ffe39f42b1e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75cd23a81b5f4049b02e0824a6fc527c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0f047609e04e779cc5e994a12d5e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe738ac323b4a118d517e76fd7228d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4452796f164e414aa72d414ac70b2aea"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12859 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75391c7cff54aff8f8de778e105feee"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'chosen': 'Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.<|im_end|>\\n',\n 'rejected': ' Sure! Here\\'s a sentence that describes all the data you provided:\\n\\n\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\"<|im_end|>\\n',\n 'prompt': '<|im_start|>system\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|im_end|>\\n<|im_start|>user\\nGenerate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One<|im_end|>\\n<|im_start|>assistant\\n'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train model with DPO","metadata":{"id":"DeT5eUK_UJgK"}},{"cell_type":"code","source":"# LoRA configuration\npeft_config = LoraConfig(\n    r=1,\n    lora_alpha=1,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['k_proj'] # , 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj'\n)\n\n# Model to fine-tune\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True\n)\nmodel.config.use_cache = False\n\n# Reference model\nref_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    max_steps=200,\n    save_strategy=\"no\",\n    logging_steps=1,\n    output_dir=new_model,\n    optim=\"paged_adamw_32bit\",\n    warmup_steps=100,\n    fp16=True,\n    report_to=\"wandb\",\n)\n\n# Create DPO trainer\ndpo_trainer = DPOTrainer(\n    model,\n    ref_model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    beta=0.1,\n    max_prompt_length=1024,\n    max_length=1536,\n)\n\n# Fine-tune model with DPO\ndpo_trainer.train()","metadata":{"id":"rKPILNOLR-aK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upload model","metadata":{"id":"3LdhPpcrUM3H"}},{"cell_type":"code","source":"# Save artifacts\ndpo_trainer.model.save_pretrained(\"final_checkpoint\")\ntokenizer.save_pretrained(\"final_checkpoint\")\n\n# Flush memory\ndel dpo_trainer, model, ref_model\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:47:26.247044Z","iopub.execute_input":"2024-02-15T14:47:26.247757Z","iopub.status.idle":"2024-02-15T14:47:26.875836Z","shell.execute_reply.started":"2024-02-15T14:47:26.247723Z","shell.execute_reply":"2024-02-15T14:47:26.874705Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Reload model in FP16 (instead of NF4)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    return_dict=True,\n    torch_dtype=torch.float16,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Merge base model with the adapter\nmodel = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\nmodel = model.merge_and_unload()\n\n# Save model and tokenizer\nmodel.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)","metadata":{"id":"h7cIvxcTfBC4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Push them to the HF Hub\nnew_model = 'shahzebnaveed/NeuralHermes-2.5-Mistral-7B'\nprint(new_model)\nmodel.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\ntokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-02-15T14:52:31.223267Z","iopub.execute_input":"2024-02-15T14:52:31.223642Z","iopub.status.idle":"2024-02-15T14:57:01.874006Z","shell.execute_reply.started":"2024-02-15T14:52:31.223612Z","shell.execute_reply":"2024-02-15T14:57:01.872992Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"shahzebnaveed/NeuralHermes-2.5-Mistral-7B\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2afc0d65069847b29c89eb7eaf2ef121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32bdf933b32428280d007a04fe8dfaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63763aed32f2408e9c9fbea0a89db02a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e4c391ac90541d79a7e5807998d99ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f25385f5dd4a5e922b3170cab899e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376442749a0b484597c6037a9c57888a"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shahzebnaveed/NeuralHermes-2.5-Mistral-7B/commit/383999b095899d26168b496de8cb3d1100e1f168', commit_message='Upload tokenizer', commit_description='', oid='383999b095899d26168b496de8cb3d1100e1f168', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"G6EFsmS4UOgV"}},{"cell_type":"code","source":"# Format prompt\nmessage = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n]\ntokenizer = AutoTokenizer.from_pretrained(new_model)\nprompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n\n# Create pipeline\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=new_model,\n    tokenizer=tokenizer\n)\n\n# Generate text\nsequences = pipeline(\n    prompt,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    num_return_sequences=1,\n    max_length=200,\n)\nprint(sequences[0]['generated_text'])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251,"referenced_widgets":["22773c721a7c4221a9c14cd388461d4c","6b54841f5de1482694c360095dae3039","448ccbc85e624ec3b3e71931a7ee4ff6","173769f6f465485f8848a11bf269850b","60978b9b4e8348f0a71ce3e35c73bcff","6a38dcbaf4674b448329ac0a16587d2a","7eaeada2158e493189449af91f643553","6e32854952b340008edca0139d3471d6","db6d7cfcdade4b4baa213a5d0abc07d7","9083029642744c43b7705532cbe0cf79","d028a98caa13425b907ceb513119006e"]},"id":"LAEUZFjvlJOv","outputId":"9b5720c7-49ef-45c7-e5a7-f38d64899b1e","execution":{"iopub.status.busy":"2024-02-15T15:09:00.822623Z","iopub.execute_input":"2024-02-15T15:09:00.823435Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2757e082d01641d4bf701c852b422bb6"}},"metadata":{}}]}]}